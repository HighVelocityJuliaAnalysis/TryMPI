{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating $e$ with parallel code\n",
    "\n",
    "Recall in the `1-Nonparallel` notebook, we found a way to calculate $e$ with a sum...\n",
    "\n",
    "$$e = \\sum^\\infty_{i=0} \\frac{1}{i!}$$\n",
    "\n",
    "We wrote some julia code to do this...\n",
    "\n",
    "```julia\n",
    "calculateTerm(i) = 1.0/factorial(i)\n",
    "calculate_e(nTerms) = [ calculateTerm(i) for i in 0:nTerms ] |> sum\n",
    "```\n",
    "\n",
    "We executed this code in a non-parallel program, that is $e$ was calculated serially in on process by one processor.\n",
    "\n",
    "With MPI, we can have terms calculated simultaneously with parallel processes. The simplest way to do this is to have each rank calculate a term and then add them all up. So the number of terms in the sum will be the same as the number of ranks. An interesting challenge with this code is how to do the sum? Each rank will only know the value of its term. We will need to use MPI communication in order to have each rank communicate its term to a *Root rank*. The root rank is a special rank that will collect information from the other ranks and process it - in this case, the root rank will sum up the terms. MPI doesn't care which rank is the root rank, but it is customary to choose Rank 0 as the root. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Development/HighVelocityJuliaAnalysis/TryMPI/notebooks`\n"
     ]
    }
   ],
   "source": [
    "# This cell should add the packages you need for \n",
    "# this notebook (and other notebooks in this directory)\n",
    "# You should get a message saying \n",
    "# \"Activating project at `.../TryMPI/notebooks` where the ... is the \n",
    "# absolute path to the directory of this repository and these notebooks.\n",
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "Pkg.instantiate()\n",
    "\n",
    "# This cell will appear in other notebooks as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are actually two ways we can do the sum. 1) We can use the `MPI.Gather` function to pass an array to the Root rank and have that rank do the sum. Or 2) we can use `MPI.Reduce` to have MPI calculate the sum for us. Let's look at both ways. \n",
    "\n",
    "See https://mpitutorial.com/tutorials/mpi-scatter-gather-and-allgather/ for a good tutorial on `MPI.Gather` (but the code is C instead of Julia). \n",
    "\n",
    "### Gather and sum\n",
    "\n",
    "Look at `calc_e_gather.jl` . You can run it from the terminal with\n",
    "\n",
    "```bash\n",
    "~/.julia/bin/mpiexecjl --project -n 4 julia calc_e_gather.jl \n",
    "```\n",
    "\n",
    "There are some new lines compared to what you saw in `displayRank.jl`.\n",
    "\n",
    "```julia\n",
    "rootRank = 0\n",
    "iAmRoot = myRank == rootRank\n",
    "```\n",
    "\n",
    "Here we are saying that the root rank is Rank 0. We could have picked any rank, but 0 is a good choice because even an MPI program with only one rank will always have a Rank 0. The `iAmRoot` line is just an easy short hand to determine if this rank is the root rank. Remember that MPI runs copies of this code simultaneously. Only one copy will be Rank 0 and thus only that copy will have `iAmRoot` as `true`. The other ranks will have `iAmRoot` as `false`. \n",
    "\n",
    "```julia\n",
    "# Function for calculating a term\n",
    "calculateTerm(i) = 1.0/factorial(i)\n",
    "\n",
    "# Calculate my term\n",
    "myTerm = calculateTerm(myRank)\n",
    "```\n",
    "\n",
    "Here we define the function to calculate a term of $e$ and then we use it. Note that we're using the rank as $i$ in the function. This is convenient, as rank is an integer that starts with zero. \n",
    "\n",
    "```julia\n",
    "# Gather terms to the Root rank\n",
    "terms = MPI.Gather(myTerm, rootRank, comm)\n",
    "\n",
    "print(\"I am rank $(myRank) and terms are $(terms)\\n\")\n",
    "```\n",
    "\n",
    "This is where the action happens. We give `MPI.Gather` our value, which Rank is root (in this case 0), and the communicator. The value of `terms` depends on the rank.\n",
    "- The Root Rank (rank 0) will get an array. The ith entry will be the term from the (i-1)th rank. That is entry 1 in the array is the term from rank 0. Entry 2 in the array is the term from rank 1 (here is where Julia's 1-based array indexing gets annoying).\n",
    "- The other ranks will have `term` set to `nothing`. `nothing` is a special Julia value that means what it says - it is nothing. You can't do any operations on it, except to compare (e.g. `term == nothing` would be true).\n",
    "\n",
    "The `print` line verifies the above. \n",
    "\n",
    "```julia\n",
    "if iAmRoot\n",
    "    e_estimate = sum(terms)\n",
    "    print(\"Estimate of e with $(nRanks) terms is $(e_estimate)\\n\")\n",
    "end\n",
    "```\n",
    "\n",
    "This last part only runs in the root rank (the other ranks will be idle). It calculates the sum and prints it, giving our estimate of $e$. Can you think of why this code must only run in the root rank (e.g. why the `if` statement)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce\n",
    "\n",
    "Look at `calc_e_reduce.jl` and try to run it. It looks very similar to the program we just examined, `calc_e_gather.jl`, but with a big difference. Instead of gathering all of the terms to the Root Rank and having that rank calculate the sum, MPI actually calculates the sum for us with the `MPI.Reduce` function call. \n",
    "\n",
    "```julia\n",
    "e_estimate = MPI.Reduce(myTerm, +, rootRank, comm)\n",
    "\n",
    "print(\"I am rank $(myRank) and reduce returned $(e_estimate)\\n\")\n",
    "```\n",
    "\n",
    "For `MPI.Reduce`, each rank passes in a value (`myTerm` in this case) and MPI *reduces* those values by running the specified operation (`+` in this case). The result of those operations is one number, and that is returned to the Root Rank. The other ranks get the Julia `nothing` value (just like with `MPI.Gather`).\n",
    "\n",
    "```julia\n",
    "if iAmRoot\n",
    "    print(\"The estimate for e is $(e_estimate)\")\n",
    "end\n",
    "```\n",
    "\n",
    "Here, we print the result nicely. Why do we need the `if` statement?\n",
    "\n",
    "You may be wondering how we can use `+` as a function in the `MPI.Reduce` call. `+` is actually a Julia function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "+(2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`+` has a special *infix* form where `a + b` gets interpreted as `+(a,b)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some advantages of using `MPI.Reduce` over `MPI.Gather`. With `MPI.Gather`, we returned an array of terms to the Root rank, and then that rank did the sum. That sum happened serially. If we had an extremely large list of terms, then that sum could have taken a long time to compute. Having `MPI.Reduce` do the sum means that the sum happens in parallel on many processors and this can be much more efficient than having one rank do the sum itself. The MPI reduce [tutorial](https://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/) explains this pretty well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
